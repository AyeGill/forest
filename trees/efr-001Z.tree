\date{2024-04-30}
\author{eigil-rischel}
\title{Convex Duality made Difficult}
\import{macros}
%Introduction

%Convex optimization problems
\subtree{
  \title{Convex optimization}

  %def: std. form convex
  \transclude{efr-001G}
  %def: Lagrangian
  \transclude{efr-001K}

  \p{
    Observe that #{\sup_{\lambda,\nu} L(x,\lambda,\nu)} is #{f_0(x)} if #{x} satisfies the constraints of the problem, and #{\infty} otherwise. Hence we can think of this minimization problem as playing a zero-sum game: we choose #{x}, our adversary chooses #{\lambda,\nu}, and our loss function is #{L}.
  }
  \p{
    It is natural to ask about the existence of Nash equilibria in this game - observe that the existence of an equilibrium #{(x^*,\lambda^*,\nu^*)} means that #{\inf_x \sup_{\lambda,\nu} L(x,\lambda,\nu) = \sup_{\lambda,\nu} \inf_x L(x,\lambda,\nu) = L(x^*,\lambda^*,\nu^*)}. This is of great utility in solving the original problem.
  }
  \p{
    The \em{dual problem} is the problem of \em{maximizing} the function #{\inf_x L(x,\lambda,\nu)}. This is always a concave problem.
  }
  \p{
    In the world of convex optimization, two problems whose constraints carve out the same subset of #{\RR^k} would be called \em{equivalent}. But they can clearly not be regarded as \em{isomorphic}, because the choice of constraint functions makes an important difference to the theory of optimization (for example, it can lead to different dual problems). Here we take the viewpoint that the \em{lagrangian} is really the fundamental object in convex optimization - by passing to a suitable category of Lagrangians, we can make the dual problem into an actual self-duality on this category.
  }
}

\subtree{
  \title{Convex spaces}
  \transclude{efr-0020}

  \transclude{efr-002N}
  \transclude{efr-002M}
  \p{
    The term "convex function" in this sense clashes with the usual practice of naming structure-preserving functions after the structure they preserve (since convex functions do not preserve the convex structure). Unfortunately this usage is far too established to alter. (Convex functions are called convex because they are exactly those functions where the area above their graph is a convex subset of #{X \times \RR}. Although there appears to be no particular reason why the terms convex and concave should not be interchanged, other than convention).
  }
  \transclude{efr-002O}
  \p{
    Justified by \ref{efr-002O}, we will appropriate the term \em{affine} to refer to #{\Delta}-homomorphisms, even between convex spaces which are not vector spaces. There is generally no chance of confusion, but it's worth emphasizing that the use of this term does not entail that the domain is closed under arbitrary affine combinations, for example.
  }
  
  \p{Convex spaces admit both a Cartesian product (given by the product of the underlying sets equipped with pointwise operations) and a tensor product, which (co)represents "bihomomorphisms". This is analogous to the situation for vector spaces. Unlike vector spaces, however, since all constant maps are homomorphisms, the projections #{X \times Y \to X,Y} are bihomomorphisms, which induces a map #{X \otimes Y \to X \times Y}. Thus homomorphisms #{X \times Y \to Z} are a subset of bihomomorphisms.}

  \p{Since we are generally dealing with convex or concave functions, which can't freely be extended to the tensor product, we will work with the Cartesian product in this paper. But it's very possible that most of our constructions would work also with the tensor product, and maybe there is some situation where the extra generality is necessary.}
}

\subtree{
  \title{The Category of Minmax problems}
  %def: minmax
  \transclude{efr-001C}

  \p{We will see that various constructions on this category, which are natural and well-behaved from the point of view of category theory, capture relevant constructions from the theory of convex optimization.}

  \ol{
    \li{#{\Minmax} is bifibred over #{\Set^\Delta \times \Set^{\Delta,\op}}, and the Cartesian and coCartesian lifts capture the operations of minimizing over the primal variables or maximizing over the dual variables}
    \li{The property of \em{strong duality} amounts to the claim that a particular diagram has the local Beck-Chevalley property}
    \li{Relatedly, the existence of a Nash equilibrium for the game corresponding to #{L} amounts to the existence of a certain morphism. The fact that this implies strong duality can be derived by purely categorical means.}
  }

  %Relation to ordinary ones
  \transclude{efr-002H}
  \p{
    Observe that minmax problems affine in #{A} are thus very similar to standard-form convex optimization problems, the main difference being that the set of allowed points in #{A} may be constrained in some other way than by requiring certain coordinates to be nonnegative.
  }
  \p{
    On the other hand, if #{A} is thus constrained, the proposition doesn't actually imply that #{f,g} are convex! The easiest way to see this is by considering #{A = \{a\}} for some nonzero #{a}. Then we have #{L(x,a) = f(x) + ag(x),}, and clearly we can choose this decomposition in such a way that these functions are not convex.
  }
  \p{
    However, if #{A \subset \RR^m} contains the positive cone #{\RR^m_+,} for example, we do have both #{f} and all the coordinates of #{g} convex.
  }

  %Primal dual problem
  \transclude{efr-001D}

  % Duality

  \transclude{efr-001J}

  \p{
    We will often utilize this duality to abbreviate proofs, proving something, for example, for the forwards direction and arguing "by duality" that it holds for the backwards direction as well.
  }


  %backwards/forwards maps
  \transclude{efr-0027}


  %fibration structure (min-maxing)
  
  \transclude{efr-002A}

  \transclude{efr-0023}

  \p{
    What's "really" going on here is that #{\Minmax} is a two-sided fibration, the result of taking the functor #{\Set^{\Delta,\op} \times \Set^{\Delta,\op} \to \Cat} carrying a pair #{X,Y} to the poset of minmax problems #{L: X \times Y \to \RR} (in the opposite order), with morphisms acting by precomposition, and applying the Grothendieck construction "contravariantly in the first variable and covariantly in the second variable". (And then observing that the precomposition action has left/right adjoints given by #{\inf}/#{\sup}, to make this into a \em{bi}fibration). But the theory of bifibrations is quite complicated in general, and we will not go into it here.
  }

  \p{
    Note also that this functor is quite close to displaying #{\Minmax} as \em{topological}. If we remove the restriction that minmax problems be convex/concave, we can construct the universal lifts required using a similar supremum formula. The problem is that the supremum of a general set of concave functions is not automatically concave (however, the supremum taken over a convex set, in a suitable sense, is).
  }
  

  %Conv/Conc
  \transclude{efr-001R}
  \transclude{efr-001S}
  \p{
    Note that if #{\phi = (\phi^+,\phi^-): L \to L'} is a morphism of #{\Minmax}, the two meanings of the notation #{\phi^+} agree, and the same is true of #{\phi^-}.
  }
  \p{
    Note also that the reflexive subcategory #{\Conc^\op \subseteq \Minmax} is the local subcategory with respect to the forwards morphisms - a morphism is forward if and only if #{\phi^-} is an isomorhism (by definition), and the unit #{L \to L^-} is the terminal forwards morphism with domain #{L}. A dual statement holds for #{\Conv \subseteq \Minmax} (it is the \em{colocalization} with respect to the class of backwards morphisms).
  }


  %monoidal structure
  \transclude{efr-001N}
  %monoidal fibration
  \transclude{efr-002D}
  \p{In the case of a Cartesian base, a monoidal fibration (like the one we have here) is equivalent to a fibration with a monoidal structure on each fiber, compatible with the reindexing in a certain way. Our base is not Cartesian, but does seem to come from a monoidal structure on each fiber, given by addition of #{L}s. The point is that, given #{(X,A,L)}, there is a canonical way to obtain an #{L} on #{(X\times Y, A \times B)}, given by using a Cartesian lift of #{X \times Y \to X} and a \em{coCartesian} lift of #{A \times B \to A}. This suggests there should be a useful theory of \em{monoidal two-sided fibrations}, but this notion does not appear to have been studied before.}
  
  %monoidal localization
  \transclude{efr-002P}
}



\subtree{
  \title{Strong duality}
  \transclude{efr-001W}

  %Definition:
  \transclude{efr-002F}

  %Slater, classical
  \transclude{efr-002G}
  \p{
    This theorem can be proven by entirely classical methods. However, we will derive it from a more abstract statement about minmax problems.
  }
  %Slater, my version
}

\subtree{
  \title{The Legendre Transform}
  
  \transclude{efr-001P}
  \transclude{efr-001Q}
  \transclude{efr-001X}
  \transclude{efr-001U}
  \transclude{efr-001T}

  \transclude{efr-001V}

  \p{
    The strong duality in problems of the form #{f|_0} is a (very) special case of the strong duality necessary to make composition well-defined in a double category of minmax problems, see [[efr-001Y]]. In terms of such a double category, we can say that the theorem #{(f^*)^* = f} (for convex #{f}) is a case of the unitality of composition.
  }
}


\subtree{
  \title{Misc stuff (sorting)}
  \transclude{efr-001Y}
  \transclude{efr-001M}
}